{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265dc2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_page_token CKwCEAA\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import requests\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "API_KEY = 'XXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "def save_next_page_token(token, filename='next_page_token.json'):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump({\"nextPageToken\": token}, file)\n",
    "\n",
    "def load_next_page_token(filename='next_page_token.json'):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data.get(\"nextPageToken\")\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def tags_to_csv():\n",
    "    \"\"\"\n",
    "    Récupère les vidéos correspondant aux tags définis, puis enregistre les métadonnées dans un fichier CSV.\n",
    "    \"\"\"\n",
    "    tags = [\n",
    "        \"Cloud Computing\", \"Kubernetes\", \"Docker\", \"DevOps\", \"Machine Learning\", \"AI\", \"Data Science\", \"Big Data\"\n",
    "    ]\n",
    "\n",
    "    # Récupérer les identifiants des vidéos\n",
    "    video_ids = get_technology_video_ids(tags, max_results=50)\n",
    "\n",
    "    # Écrire les informations des vidéos dans un fichier CSV\n",
    "    with open('video_info.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['video_id', 'title', 'publishedAt', 'channelId', 'description', 'view_count', \n",
    "                         'like_count', 'favorite_count', 'comment_count', 'tags', 'defaultLanguage'])\n",
    "\n",
    "        for video_id in video_ids:\n",
    "            video_info = get_video_info(video_id)\n",
    "            if video_info:\n",
    "                writer.writerow([\n",
    "                    video_id,\n",
    "                    video_info['snippet']['title'],\n",
    "                    video_info['snippet']['publishedAt'],\n",
    "                    video_info['snippet']['channelId'],\n",
    "                    video_info['snippet']['description'],\n",
    "                    video_info['statistics'].get('viewCount', 'N/A'),\n",
    "                    video_info['statistics'].get('likeCount', 'N/A'),\n",
    "                    video_info['statistics'].get('favoriteCount', 'N/A'),\n",
    "                    video_info['statistics'].get('commentCount', 'N/A'),\n",
    "                    \", \".join(video_info['snippet'].get('tags', [])),\n",
    "                    video_info['snippet'].get('defaultLanguage', 'N/A')\n",
    "                ])\n",
    "\n",
    "def get_technology_video_ids(tags, max_results=50):\n",
    "    \"\"\"\n",
    "    Récupère les identifiants de vidéos YouTube correspondant à une liste de tags et une catégorie.\n",
    "\n",
    "    Args:\n",
    "        tags: Une liste de chaînes de caractères représentant les tags à rechercher.\n",
    "        max_results: Le nombre maximum de résultats à récupérer.\n",
    "\n",
    "    Returns:\n",
    "        Une liste d'identifiants de vidéos.\n",
    "    \"\"\"\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEY, cache_discovery=False)\n",
    "\n",
    "    all_video_ids = []\n",
    "    next_page_token = load_next_page_token() \n",
    "\n",
    "    # Diviser les tags en sous-listes de 10 tags maximum\n",
    "    tag_groups = [tags[i:i + 10] for i in range(0, len(tags), 10)]\n",
    "\n",
    "    for tag_group in tag_groups:\n",
    "        while True:\n",
    "            try:\n",
    "                search_response = youtube.search().list(\n",
    "                    part=\"snippet\",\n",
    "                    maxResults=min(max_results, 50),  # Limite à 50 résultats par requête\n",
    "                    type=\"video\",\n",
    "                    videoCategoryId=\"28\",  # Catégorie Technologie\n",
    "                    q=\" \".join(tag_group),\n",
    "                    pageToken=next_page_token\n",
    "                ).execute()\n",
    "\n",
    "                video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
    "                all_video_ids.extend(video_ids)\n",
    "\n",
    "                next_page_token = search_response.get('nextPageToken')\n",
    "                print('next_page_token', next_page_token)\n",
    "                save_next_page_token(next_page_token)\n",
    "                if not next_page_token or len(all_video_ids) >= max_results:\n",
    "                    break\n",
    "            except HttpError as err:\n",
    "                print(f\"Erreur lors de la récupération des vidéos : {err}\")\n",
    "                break\n",
    "\n",
    "    return all_video_ids[:max_results]\n",
    "\n",
    "\n",
    "def get_video_info(video_id):\n",
    "    \"\"\"\n",
    "    Récupère les informations détaillées d'une vidéo YouTube à partir de son identifiant.\n",
    "\n",
    "    Args:\n",
    "        video_id: L'identifiant de la vidéo.\n",
    "\n",
    "    Returns:\n",
    "        Un dictionnaire contenant les informations de la vidéo, ou None en cas d'erreur.\n",
    "    \"\"\"\n",
    "    url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    params = {\n",
    "        \"id\": video_id,\n",
    "        \"part\": \"snippet,statistics\",\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['items'][0]\n",
    "    else:\n",
    "        print(f\"Erreur pour l'ID vidéo {video_id}: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "tags_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f11fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def clean_tags_videos():\n",
    "    # Dictionnaire de mappage\n",
    "    tag_mapping = {\n",
    "        \"ml\": \"machine learning\",\n",
    "        \"ai\": \"artificial intelligence\",\n",
    "        \"devops\": \"dev ops\",\n",
    "        \"datascience\": \"data science\",\n",
    "        \"gcp\": \"google cloud platform\",\n",
    "        \"kuebernetes\": \"kubernetes\",\n",
    "        \"kubeflow\": \"kubeflow\",\n",
    "        \"bigdata\": \"big data\",\n",
    "        \"bigdataldn\": \"big data london\",\n",
    "        \"eks\": \"amazon elastic kubernetes service\",\n",
    "        \"aks\": \"azure kubernetes service\",\n",
    "        \"mlops\": \"machine learning operations\",\n",
    "        \"tf\": \"tensorflow\",\n",
    "        \"gke\": \"google kubernetes engine\",\n",
    "        \"does\": \"devops enterprise summit\",\n",
    "        \"does19\": \"devops enterprise summit 2019\",\n",
    "        \"ndc\": \"norwegian developer conference\",\n",
    "        \"odsc\": \"open data science conference\",\n",
    "        \"sparkaisummit\": \"apache spark ai summit\",\n",
    "        \"docker container\": \"docker\",\n",
    "        \"containerization\": \"containers\",\n",
    "        \"cloudbased platform\": \"cloud platform\",\n",
    "        \"machine language models\": \"machine learning models\",\n",
    "        \"no tags\": \"\",\n",
    "        \"type conference talk full production\": \"conference\",\n",
    "        \"prpr google cloud next\": \"google cloud next\",\n",
    "        \"purpose educate\": \"education\",\n",
    "        \"app development\": \"application development\",\n",
    "        \"backend devlopment\": \"backend development\",\n",
    "        \"kubernetes explained\": \"kubernetes\",\n",
    "        \"cloud onair\": \"cloud\",\n",
    "        \"it infrastructure\": \"infrastructure\",\n",
    "        \"webdev\": \"web development\",\n",
    "        \"lesson\": \"tutorial\",\n",
    "        \"tutorialspoint\": \"tutorial\",\n",
    "        \"grok 3 cosmic diplomacy\": \"grok\",\n",
    "        \"xspeculation alliances\": \"alliances\",\n",
    "        \"3000 ai sims\": \"ai simulations\",\n",
    "        \"interstellar prompt tools\": \"prompt tools\",\n",
    "        \"advanced diplomacy reasoning\": \"diplomacy\",\n",
    "        \"cosmic social dynamics\": \"social dynamics\",\n",
    "        \"grokai\": \"grok\",\n",
    "        \"prompting\": \"prompt\",\n",
    "        \"promptengineering\": \"prompt engineering\",\n",
    "        \"all things open\": \"open source\",\n",
    "        \"ato2020\": \"all things open 2020\",\n",
    "        \"technology\": \"tech\",\n",
    "        \"tania allard phd\": \"tania allard\",\n",
    "        \"microsoftnshivani patel\": \"shivani patel\",\n",
    "        \"aws cloud\": \"amazon web services\",\n",
    "        \"cloud computing explained\": \"cloud computing\",\n",
    "        \"docker tutorial\": \"docker\",\n",
    "        \"docker container\": \"docker\",\n",
    "        \"docker containers explained\": \"docker\",\n",
    "        \"singularity container\": \"singularity\",\n",
    "        \"cloud computing tutorial for beginners\": \"cloud computing\",\n",
    "        \"docker vs kubernetes\": \"docker and kubernetes\",\n",
    "        \"kuebernetes tutorials\": \"kubernetes tutorials\",\n",
    "        \"container tutorial\": \"containers\",\n",
    "        \"natural language processsing\": \"natural language processing\",\n",
    "        \"machine vision\": \"computer vision\",\n",
    "        \"cloud\": \"cloud computing\",\n",
    "        \"salman iqbal\": \"salman iqbal\",\n",
    "        \"ndc conferences 2021 oslo live\": \"ndc oslo 2021\",\n",
    "        \"ai conference\": \"artificial intelligence conference\",\n",
    "        \"virtual conference\": \"virtual event\",\n",
    "        \"containers\": \"containerization\",\n",
    "        \"linux kernel\": \"linux\",\n",
    "        \"cgroups\": \"control groups\",\n",
    "        \"cloud foundry\": \"cloud\",\n",
    "        \"runtimes\": \"runtime\",\n",
    "        \"nodejs\": \"node.js\",\n",
    "        \"vms\": \"virtual machines\",\n",
    "        \"host os\": \"host operating system\",\n",
    "        \"hypervisor\": \"virtualization\",\n",
    "        \"guest os\": \"guest operating system\",\n",
    "        \"agile\": \"agile methodology\",\n",
    "        \"portability\": \"portable\",\n",
    "        \"scalability\": \"scalable\",\n",
    "        \"ibm cloud kubernetes service\": \"ibm cloud\",\n",
    "        \"manifest\": \"manifest file\",\n",
    "        \"yaml\": \"yaml\",\n",
    "        \"image\": \"docker image\",\n",
    "        \"runtime engine\": \"runtime\",\n",
    "        \"docker engine\": \"docker\",\n",
    "        \"cognitive api\": \"cognitive services\",\n",
    "        \"watson\": \"ibm watson\",\n",
    "        \"python\": \"programming\",\n",
    "        \"cloudnative\": \"cloud native\",\n",
    "        \"architecture\": \"software architecture\",\n",
    "        \"continuous integration\": \"ci\",\n",
    "        \"continuous delivery\": \"cd\",\n",
    "        \"aws\": \"amazon web services\",\n",
    "        \"eks\": \"amazon eks\",\n",
    "        \"container day\": \"container event\",\n",
    "        \"big data ldn\": \"big data london\",\n",
    "        \"bigdata\": \"big data\",\n",
    "        \"conference\": \"event\",\n",
    "        \"big data expo\": \"big data exhibition\",\n",
    "        \"big data\": \"big data analytics\",\n",
    "        \"100 seconds of code\": \"coding tutorial\",\n",
    "        \"backend development\": \"backend\",\n",
    "        \"kubernetes architecture\": \"kubernetes\",\n",
    "        \"linux\": \"operating system\",\n",
    "        \"virtualization\": \"virtual machines\",\n",
    "        \"linux on the desktop\": \"linux desktop\",\n",
    "        \"hypervisors\": \"virtualization\",\n",
    "        \"embedded systems\": \"embedded\",\n",
    "        \"linux containers\": \"containers\",\n",
    "        \"linux security\": \"security\",\n",
    "        \"runai\": \"run ai\",\n",
    "        \"anaconda\": \"data science platform\",\n",
    "        \"brainchip\": \"ai hardware\",\n",
    "        \"devoxxbe\": \"devoxx belgium\",\n",
    "        \"devoxxbe19\": \"devoxx belgium 2019\",\n",
    "        \"microsoft azure kubernetes\": \"azure kubernetes service\",\n",
    "        \"aks cluster\": \"azure kubernetes service\",\n",
    "        \"aks devops\": \"azure devops\",\n",
    "        \"data engineer\": \"data engineering\",\n",
    "        \"dataanalyst\": \"data analyst\",\n",
    "        \"engineer\": \"software engineer\",\n",
    "        \"databricks\": \"data platform\",\n",
    "        \"deep learning\": \"dl\",\n",
    "        \"mlops\": \"machine learning operations\",\n",
    "        \"tf serving\": \"tensorflow serving\",\n",
    "        \"sagemaker\": \"aws sagemaker\",\n",
    "        \"pytorch\": \"deep learning framework\",\n",
    "        \"tensorflow\": \"ml framework\",\n",
    "        \"onnx\": \"open neural network exchange\",\n",
    "        \"model deployment\": \"deployment\",\n",
    "        \"workshop\": \"training\",\n",
    "        \"sql server\": \"microsoft sql server\",\n",
    "        \"sql server 2019\": \"sql server\",\n",
    "        \"big data clusters\": \"big data cluster\",\n",
    "        \"polybase\": \"data integration\",\n",
    "        \"enhanced polybase\": \"polybase\",\n",
    "        \"data virtualization\": \"virtualization\",\n",
    "        \"database administrator\": \"dba\",\n",
    "        \"dba\": \"database administration\",\n",
    "        \"devops enterprise summit usa\": \"devops enterprise summit\",\n",
    "        \"jordan edwards\": \"speaker\",\n",
    "        \"microsoftnshivani patel\": \"speaker\",\n",
    "        \"machine learning\": \"ml\",\n",
    "        \"data science\": \"ds\",\n",
    "        \"datascience\": \"data science\",\n",
    "        \"odsc\": \"open data science conference\",\n",
    "        \"docker\": \"containerization\",\n",
    "        \"databricks\": \"data platform\",\n",
    "        \"how to use kubeflow\": \"kubeflow tutorial\",\n",
    "        \"what is kubeflow\": \"kubeflow\",\n",
    "        \"google kubernetes engine\": \"gke\",\n",
    "        \"google compute engine\": \"gce\",\n",
    "        \"machine language models\": \"ml models\",\n",
    "        \"python tutorial\": \"python\",\n",
    "        \"docker container\": \"docker\",\n",
    "        \"ml models\": \"machine learning models\",\n",
    "        \"gke\": \"google kubernetes engine\",\n",
    "        \"python\": \"programming language\",\n",
    "        \"fairing\": \"kubeflow fairing\",\n",
    "        \"google cloud ai huddle\": \"google ai\",\n",
    "        \"google\": \"google cloud\",\n",
    "        \"karthik ramasamy\": \"author\",\n",
    "        \"gds yes\": \"global data science\",\n",
    "        \"sparkaisummit\": \"apache spark ai summit\",\n",
    "        \"apache hadoop\": \"hadoop\",\n",
    "        \"apache spark\": \"spark\",\n",
    "        \"docker  container\": \"docker\"\n",
    "    }\n",
    "\n",
    "    # Charger le fichier\n",
    "    file_path = \"video_info.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convertir 'publishedAt' en datetime\n",
    "    # df[\"publishedAt\"] = pd.to_datetime(df[\"publishedAt\"], format=\"%d%m%Y\")\n",
    "    df[\"publishedAt\"] = pd.to_datetime(df[\"publishedAt\"], utc=True)\n",
    "\n",
    "    # Ajouter des colonnes pour le filtrage\n",
    "    df[\"year\"] = df[\"publishedAt\"].dt.year  # Année\n",
    "    df[\"month\"] = df[\"publishedAt\"].dt.strftime(\"%m\")\n",
    "    df[\"month_name\"] = df[\"publishedAt\"].dt.strftime(\"%B\")\n",
    "\n",
    "    # Catégorisation des vidéos par popularité\n",
    "    bins = [0, 1000, 10000, 100000, 1000000, float(\"inf\")]\n",
    "    labels = [\"0-1K\", \"1K-10K\", \"10K-100K\", \"100K-1M\", \"1M+\"]\n",
    "    df[\"popularity_category\"] = pd.cut(df[\"view_count\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Remplacement des valeurs NaN pour éviter les erreurs\n",
    "    df[\"like_count\"].fillna(0, inplace=True)\n",
    "    df[\"comment_count\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Nettoyage des tags pour enlever les caractères spéciaux\n",
    "    def clean_tags(tag_list):\n",
    "        cleaned_tags = [re.sub(r'[^a-zA-Z0-9\\s]', '', tag).strip() for tag in tag_list]\n",
    "        return cleaned_tags\n",
    "\n",
    "    # Normalisation des tags avec le dictionnaire de mappage\n",
    "    def normalize_tags(tags, mapping):\n",
    "        return [mapping.get(tag.lower(), tag.lower()) for tag in tags]\n",
    "\n",
    "    # Appliquer le nettoyage et la normalisation des tags\n",
    "    df[\"tags\"] = df[\"tags\"].apply(lambda x: normalize_tags(clean_tags(x.split(\",\")), tag_mapping) if pd.notna(x) else [])\n",
    "\n",
    "    # Explosion de la colonne\n",
    "    df = df.explode(\"tags\")\n",
    "\n",
    "    # Supprimer les lignes avec des tags vides après l'explosion\n",
    "    df = df[df[\"tags\"] != \"\"]\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # # Enregistrer le fichier transformé\n",
    "    output_path = \"video_info_dashboard.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "clean_tags_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100567d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 vidéos ont été insérées dans la base MongoDB.\n"
     ]
    }
   ],
   "source": [
    "def csv_to_mongo_compass():\n",
    "    \"\"\"\n",
    "    Lit les données depuis le fichier CSV généré et les insère dans une base de données MongoDB locale.\n",
    "    \"\"\"\n",
    "    client = MongoClient(\"mongodb+srv://gaetan:05XXXXXXfR@cluster0.wxcxg.mongodb.net/\")\n",
    "    db = client['youtube_data']\n",
    "    collection = db['videos3']\n",
    "\n",
    "    # Charger les données depuis le fichier CSV\n",
    "    with open('video_info_dashboard.csv', mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        video_data = [row for row in reader]\n",
    "\n",
    "    # Insérer les données dans MongoDB\n",
    "    if video_data:\n",
    "        collection.insert_many(video_data)\n",
    "        print(f\"{len(video_data)} vidéos ont été insérées dans la base MongoDB.\")\n",
    "    else:\n",
    "        print(\"Aucune donnée trouvée dans le fichier CSV.\")\n",
    "\n",
    "csv_to_mongo_compass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1325b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_df 4261\n",
      "new_df 330\n",
      "Fusion terminée avec succès ! 4288\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les fichiers CSV\n",
    "merged_df = pd.read_csv(\"video_info_dashboard_merged.csv\")\n",
    "print(\"merged_df\", len(merged_df))\n",
    "new_df = pd.read_csv(\"video_info_dashboard.csv\")\n",
    "print(\"new_df\", len(new_df))\n",
    "\n",
    "# Concaténer les données\n",
    "merged_df = pd.concat([merged_df, new_df], ignore_index=True)\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Sauvegarder le fichier mis à jour\n",
    "merged_df.to_csv(\"video_info_dashboard_merged.csv\", index=False)\n",
    "\n",
    "print(\"Fusion terminée avec succès !\", len(merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709e2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
